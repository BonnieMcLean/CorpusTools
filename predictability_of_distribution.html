<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Predictability of Distribution &mdash; Phonological CorpusTools 1.0.0 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="top" title="Phonological CorpusTools 1.0.0 documentation" href="index.html" />
    <link rel="next" title="Kullback-Leibler Divergence" href="kullback-leibler.html" />
    <link rel="prev" title="Functional Load" href="functional_load.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="kullback-leibler.html" title="Kullback-Leibler Divergence"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="functional_load.html" title="Functional Load"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">Phonological CorpusTools 1.0.0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="predictability-of-distribution">
<span id="id1"></span><h1>Predictability of Distribution<a class="headerlink" href="#predictability-of-distribution" title="Permalink to this headline">¶</a></h1>
<div class="section" id="about-the-function">
<span id="about-pred-dist"></span><h2>About the function<a class="headerlink" href="#about-the-function" title="Permalink to this headline">¶</a></h2>
<p>Predictability of distribution is one of the common methods of determining
whether or not two sounds in a language are contrastive or allophonic.
The traditional assumption is that two sounds that are predictably
distributed (i.e., in complementary distribution) are allophonic, and
that any deviation from complete predictability of distribution means
that the two sounds are contrastive. <a class="reference internal" href="references.html#hall2009" id="id2">[Hall2009]</a>, <a class="reference internal" href="references.html#hall2012" id="id3">[Hall2012]</a> proposes a way of
quantifying predictability of distribution in a gradient fashion, using
the information-theoretic quantity of <em>entropy</em> (uncertainty), which is
also used for calculating functional load (see <a class="reference internal" href="functional_load.html#method-functional-load"><em>Method of calculation</em></a>), which can be used
to document the <em>degree</em> to which sounds are contrastive in a language.
This has been shown to be useful in, e.g., documenting sound changes
<a class="reference internal" href="references.html#hall2013b" id="id4">[Hall2013b]</a>, understanding the choice of epenthetic vowel in a languages
<a class="reference internal" href="references.html#hume2013" id="id5">[Hume2013]</a>, modeling intra-speaker variability (Thakur 2011),
gaining insight into synchronic phonological patterns (Hall &amp; Hall 2013),
and understanding the influence of phonological relations on perception
(<a class="reference internal" href="references.html#hall2009" id="id6">[Hall2009]</a>, <a class="reference internal" href="references.html#hall2014a" id="id7">[Hall2014a]</a>). See also the related measure of
Kullback-Leibler divergence (<a class="reference internal" href="kullback-leibler.html#kullback-leibler"><em>Kullback-Leibler Divergence</em></a>), which is used in <a class="reference internal" href="references.html#peperkamp2006" id="id8">[Peperkamp2006]</a>
and applied to acquisition; it is also a measure of the degree to which
environments overlap, but the method of calculation differs (especially
in terms of environment selection).</p>
<p>It should be noted that predictability of distribution and functional
load are not the same thing, despite the fact that both give a measure
of phonological contrast using entropy. Two sounds could be entirely
unpredictably distributed (perfectly contrastive), and still have either
a low or high functional load, depending on how often that contrast is
actually used in distinguishing lexical items. Indeed, for any degree of
predictability of distribution, the functional load may be either high or
low, with the exception of the case where both are 0. That is, if two
sounds are entirely predictably distributed, and so have an entropy of
0 in terms of distribution, then by definition they cannot be used to
distinguish between any words in the language, and so their functional
load, measured in terms of change in entropy upon merger, would also be 0.</p>
</div>
<div class="section" id="method-of-calculation">
<span id="method-pred-dist"></span><h2>Method of calculation<a class="headerlink" href="#method-of-calculation" title="Permalink to this headline">¶</a></h2>
<p>As mentioned above, predictability of distribution is calculated using
the same entropy formula as above, repeated here below, but with different
inputs.</p>
<p>Entropy:</p>
<p><span class="math">\(H = -\sum_{i \in N} p_{i} * log_{2}(p_{i})\)</span></p>
<p>Because predictability of distribution is determined between exactly two
sounds, <em>i</em> will have only two values, that is, each of the two sounds.
Because of this limitation to two sounds, entropy will range in these
situations between 0 and 1. An entropy of 0 means that there is 0
uncertainty about which of the two sounds will occur; i.e., they are
perfectly predictably distributed (commonly associated with being
allophonic). This will happen when one of the two sounds has a probability
of 1 and the other has a probability of 0. On the other hand, an entropy
of 1 means that there is complete uncertainty about which of the two
sounds will occur; i.e., they are in perfectly overlapping distribution
(what might be termed “perfect” contrast). This will happen when each
of the two sounds has a probability of 0.5.</p>
<p>Predictability of distribution can be calculated both within an individual
environment and across all environments in the language; these two
calculations are discussed in turn.</p>
<div class="section" id="predictability-of-distribution-in-a-single-environment">
<span id="method-pred-dist-environment"></span><h3>Predictability of Distribution in a Single Environment<a class="headerlink" href="#predictability-of-distribution-in-a-single-environment" title="Permalink to this headline">¶</a></h3>
<p>For any particular environment (e.g., word-initially; between vowels;
before a [+ATR] vowel with any number of intervening consonants; etc.),
one can calculate the probability that each of two sounds can occur.
This probability can be calculated using either types or tokens, just
as was the case with functional load. Consider the following toy data,
which is again repeated from the examples of functional load, though
just the original distribution of sounds.</p>
<table border="1" class="docutils">
<colgroup>
<col width="28%" />
<col width="28%" />
<col width="21%" />
<col width="24%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head" rowspan="2">Word</th>
<th class="head" colspan="3">Original</th>
</tr>
<tr class="row-even"><th class="head">Trans.</th>
<th class="head">Type
Freq.</th>
<th class="head">Token
Freq.</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-odd"><td>hot</td>
<td>[hɑt]</td>
<td>1</td>
<td>2</td>
</tr>
<tr class="row-even"><td>song</td>
<td>[sɑŋ]</td>
<td>1</td>
<td>4</td>
</tr>
<tr class="row-odd"><td>hat</td>
<td>[hæt]</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="row-even"><td>sing</td>
<td>[sɪŋ]</td>
<td>1</td>
<td>6</td>
</tr>
<tr class="row-odd"><td>tot</td>
<td>[tɑt]</td>
<td>1</td>
<td>3</td>
</tr>
<tr class="row-even"><td>dot</td>
<td>[dɑt]</td>
<td>1</td>
<td>5</td>
</tr>
<tr class="row-odd"><td>hip</td>
<td>[hɪp]</td>
<td>1</td>
<td>2</td>
</tr>
<tr class="row-even"><td>hid</td>
<td>[hɪd]</td>
<td>1</td>
<td>7</td>
</tr>
<tr class="row-odd"><td>team</td>
<td>[tim]</td>
<td>1</td>
<td>5</td>
</tr>
<tr class="row-even"><td>deem</td>
<td>[dim]</td>
<td>1</td>
<td>5</td>
</tr>
<tr class="row-odd"><td>toot</td>
<td>[tut]</td>
<td>1</td>
<td>9</td>
</tr>
<tr class="row-even"><td>dude</td>
<td>[dud]</td>
<td>1</td>
<td>2</td>
</tr>
<tr class="row-odd"><td>hiss</td>
<td>[hɪs]</td>
<td>1</td>
<td>3</td>
</tr>
<tr class="row-even"><td>his</td>
<td>[hɪz]</td>
<td>1</td>
<td>5</td>
</tr>
<tr class="row-odd"><td>sizzle</td>
<td>[sɪzəl]</td>
<td>1</td>
<td>4</td>
</tr>
<tr class="row-even"><td>dizzy</td>
<td>[dɪzi]</td>
<td>1</td>
<td>3</td>
</tr>
<tr class="row-odd"><td>tizzy</td>
<td>[tɪzi]</td>
<td>1</td>
<td>4</td>
</tr>
<tr class="row-even"><td colspan="2">Total</td>
<td>17</td>
<td>70</td>
</tr>
</tbody>
</table>
<p>Consider the distribution of [h] and [ŋ], word-initially. In this
environment, [h] occurs in 6 separate words, with a total token frequency
of 20. [ŋ] occurs in 0 words, with, of course, a token frequency of 0.
The probability of [h] occurring in this position as compared to [ŋ],
then, is 6/6 based on types, or 20/20 based on tokens. The entropy of
this pair of sounds in this context, then, is:</p>
<p><span class="math">\(H_{types/tokens} = -[1 log_{2}(1) + 0 log_{2} (0)] = 0\)</span></p>
<p>Similar results would obtain for [h] and [ŋ] in word-final position,
except of course that it’s [ŋ] and not [h] that can appear in this environment.</p>
<p>For [t] and [d] word-initially, [t] occurs 4 words in this environment,
with a total token frequency of 21, and [d] also occurs in 4 words,
with a total token frequency of 15. Thus, the probability of [t] in
this environment is 4/8, counting types, or 21/36, counting tokens, and
the probability of [d] in this environment is 4/8, counting types, or
15/36, counting tokens. The entropy of this pair of sounds is therefore:</p>
<p><span class="math">\(H_{types} = -[(\frac{4}{8} log_{2}(\frac{4}{8}))
+ (\frac{4}{8} log_{2}(\frac{4}{8}))] = 1\)</span></p>
<p><span class="math">\(H_{types} = -[(\frac{21}{36} log_{2}(\frac{21}{36}))
+ (\frac{15}{36} log_{2}(\frac{15}{36}))] = 0.98\)</span></p>
<p>In terms of what environment(s) are interesting to examine, that is of
course up to individual researchers. As mentioned in the preface to <a class="reference internal" href="#predictability-of-distribution"><em>Predictability of Distribution</em></a>,
these functions are just tools. It would be just as possible to calculate
the entropy of [t] and [d] in word-initial environments before [ɑ],
separately from word-initial environments before [u]. Or one could
calculate the entropy of [t] and [d] that occur anywhere in a word
before a bilabial nasal...etc., etc. The choice of environment should
be phonologically informed, using all of the resources that have
traditionally been used to identify conditioning environments of interest.
See also the caveats in the following section that apply when one is
calculating systemic entropy across multiple environments.</p>
</div>
<div class="section" id="predictability-of-distribution-across-all-environments-systemic-entropy">
<span id="pred-dist-envs"></span><h3>Predictability of Distribution across All Environments (Systemic Entropy)<a class="headerlink" href="#predictability-of-distribution-across-all-environments-systemic-entropy" title="Permalink to this headline">¶</a></h3>
<p>While there are times in which knowing the predictability of distribution
within a particular environment is helpful, it is generally the case that
phonologists are more interested in the relationship between the two
sounds as a whole, across all environments. This is achieved by
calculating the weighted average entropy across all environments in which
at least one of the two sounds occurs.</p>
<p>As with single environments, of course, the selection of environments
for the systemic measure need to be phonologically informed. There are
two further caveats that need to be made about environment selection when
multiple environments are to be considered, however: (1) <strong>exhaustivity</strong> and
(2) <strong>uniqueness</strong>.</p>
<p>With regard to <strong>exhausitivity</strong>: In order to calculate the total
predictability of distribution of a pair of sounds in a language, one
must be careful to include all possible environments in which at least
one of the sounds occurs. That is, the total list of environments needs
to encompass all words in the corpus that contain either of the two
sounds; otherwise, the measure will obviously be incomplete. For example,
one would not want to consider just word-initial and word-medial positions
for [h] and [ŋ]; although the answer would in fact be correct (they have 0
entropy across these environments), it would be for the wrong reason—i.e.,
it ignores what happens in word-final position, where they <em>could</em> have had
some other distribution.</p>
<p>With regard to <strong>uniqueness</strong>: In order to get an <em>accurate</em> calculation of the
total predictability of distribution of a pair of sounds, it is important
to ensure that the set of environments chosen do not overlap with each other,
to ensure that individual tokens of the sounds are not being counted multiple
times. For example, one would not want to have both [#__] and [__i] in the
environment list for [t]/[d] while calculating systemic entropy, because
the words <em>team</em> and <em>deem</em> would appear in both environments, and the sounds
would (in this case) appear to be “more contrastive” (less predictably
distributed) than they might otherwise be, because the contrasting nature
of these words would be counted twice.</p>
<p>To be sure, one can calculate the entropy in a set of individual
environments that are non-exhaustive and/or overlapping, for comparison
of the differences in possible generalizations. But, in order to get an
accurate measure of the total predictability of distribution, the set of
environments must be both exhaustive and non-overlapping. As will be
described below, PCT will by default check whether any set of environments
you provide does in fact meet these characteristics, and will throw a
warning message if it does not.</p>
<p>It is also possible that there are multiple possible ways of developing
a set of exhaustive, non-overlapping environments. For example,
“word-initial” vs. “non-word-initial” would suffice, but so would
“word-initial” vs. “word-medial” vs. “word-final.” Again, it is up to
individual researchers to determine which set of environments makes the
most sense for the particular pheonmenon they are interested in.
See <a class="reference internal" href="references.html#hall2012" id="id9">[Hall2012]</a> for a comparison of two different sets of possible
environments in the description of Canadian Raising.</p>
<p>Once a set of exhaustive and non-overlapping environments has been
determined, the entropy in each individual environment is calculated,
as described in <a class="reference internal" href="#method-pred-dist-environment"><em>Predictability of Distribution in a Single Environment</em></a>. The frequency of each environment itself is
then calculated by examining how many instances of the two sounds
occurred in each environment, as compared to all other environments, and
the entropy of each environment is weighted by its frequency. These
frequency-weighted entropies are then summed to give the total average
entropy of the sounds across the environments. Again, this value will
range between 0 (complete predictability; no uncertainty) and 1 (complete
unpredictability; maximal uncertainty). This formula is given below; e
represents each individual environment in the exhaustive set of
non-overlapping environments.</p>
<p>Formula for systemic entropy:</p>
<p><span class="math">\(H_{total} = -\sum_{e \in E} H(e) * p(e)\)</span></p>
<p>As an example, consider [t]/[d]. One possible set of exhaustive,
non-overlapping environments for this pair of sounds is (1) word-initial
and (2) word-final. The relevant words for each environment are shown in
the table below, along with the calculation of systemic entropy from
these environments.</p>
<p>The calculations for the entropy of word-initial environments were given
above; the calculations for word-final environments are analogous.</p>
<p>To calculate the probability of the environments, we simply count up the
number of total words (either types or tokens) that occur in each
environment, and divide by the total number of words (types or tokens)
that occur in all environments.</p>
<p>Calculation of systemic entropy of [t] and [d]:</p>
<table border="1" class="docutils">
<colgroup>
<col width="7%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="20%" />
<col width="8%" />
<col width="10%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head" rowspan="2"><em>e</em></th>
<th class="head" rowspan="2"><p class="first">[t]-</p>
<p class="last">words</p>
</th>
<th class="head" rowspan="2"><p class="first">[d]-</p>
<p class="last">words</p>
</th>
<th class="head" colspan="3">Types</th>
<th class="head" colspan="3">Types</th>
</tr>
<tr class="row-even"><th class="head">H(<em>e</em>)</th>
<th class="head">p(<em>e</em>)</th>
<th class="head">p(<em>e</em>) * H(<em>e</em>)</th>
<th class="head">H(<em>e</em>)</th>
<th class="head">p(<em>e</em>)</th>
<th class="head">p(<em>e</em>) * H(<em>e</em>)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-odd"><td>[#__]</td>
<td>tot,
team,
toot,
tizzy</td>
<td>dot,
dude,
deem,
dizzy</td>
<td>1</td>
<td>(4+4)/
(8+7)
=8/15</td>
<td>0.533</td>
<td>0.98</td>
<td>(21+15)/
(36+29)
=36/65</td>
<td>0.543</td>
</tr>
<tr class="row-even"><td>[__#]</td>
<td>hot,
hat,
tot,
dot,
toot</td>
<td>hid,
dude</td>
<td>0.863</td>
<td>7/15</td>
<td>0.403</td>
<td>0.894</td>
<td>29/65</td>
<td>0.399</td>
</tr>
<tr class="row-odd"><td colspan="5">&nbsp;</td>
<td>0.533+0.403=0.936</td>
<td colspan="2">&nbsp;</td>
<td>0.543+0.399=0.942</td>
</tr>
</tbody>
</table>
<p>In this case, [t]/[d] are relatively highly unpredictably distributed
(contrastive) in both environments, and both environments contributed
approximately equally to the overall measure. Compare this to the example
of [s]/[z], shown below.</p>
<p>Calculation of systemic entropy of [s] and [z]:</p>
<table border="1" class="docutils">
<colgroup>
<col width="7%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="20%" />
<col width="8%" />
<col width="10%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head" rowspan="2"><em>e</em></th>
<th class="head" rowspan="2"><p class="first">[s]-</p>
<p class="last">words</p>
</th>
<th class="head" rowspan="2"><p class="first">[z]-</p>
<p class="last">words</p>
</th>
<th class="head" colspan="3">Types</th>
<th class="head" colspan="3">Types</th>
</tr>
<tr class="row-even"><th class="head">H(<em>e</em>)</th>
<th class="head">p(<em>e</em>)</th>
<th class="head">p(<em>e</em>) * H(<em>e</em>)</th>
<th class="head">H(<em>e</em>)</th>
<th class="head">p(<em>e</em>)</th>
<th class="head">p(<em>e</em>) * H(<em>e</em>)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-odd"><td>[#__]</td>
<td>song,
sing,
sizzle</td>
<td>&nbsp;</td>
<td>0</td>
<td>3/8</td>
<td>0</td>
<td>0</td>
<td>14/33</td>
<td>0</td>
</tr>
<tr class="row-even"><td>[__#]</td>
<td>hiss</td>
<td>his</td>
<td>1</td>
<td>2/8</td>
<td>0.25</td>
<td>0.954</td>
<td>8/33</td>
<td>0.231</td>
</tr>
<tr class="row-odd"><td>[V_V]</td>
<td>&nbsp;</td>
<td>sizzle,
dizzy,
tizzy</td>
<td>0</td>
<td>3/8</td>
<td>0</td>
<td>0</td>
<td>11/33</td>
<td>0</td>
</tr>
<tr class="row-even"><td colspan="5">&nbsp;</td>
<td>0.25</td>
<td colspan="2">&nbsp;</td>
<td>0.231</td>
</tr>
</tbody>
</table>
<p>In this case, there is what would traditionally be called a contrast word
finally, with the minimal pair <em>hiss</em> vs. <em>his</em>; this contrast is neutralized
(made predictable) in both word-initial position, where [s] occurs but
[z] does not, and intervocalic position, where [z] occurs but [s] does
not. The three environments are roughly equally probable, though the
environment of contrast is somewhat less frequent than the environments
of neutralization. The overall entropy of the pair of sounds is on
around 0.25, clearly much closer to perfect predictability (0 entropy)
than [t]/[d].</p>
<p>Note, of course, that this is an entirely fictitious example—that is,
although these are real English words, one would <strong>not</strong> want to infer
anything about the actual relationship between either [t]/[d] or [s]/[z]
on the basis of such a small corpus. These examples are simplified for
the sake of illustrating the mathematical formulas!</p>
</div>
<div class="section" id="predictability-of-distribution-across-all-environments-i-e-frequency-only-entropy">
<span id="pred-dist-all"></span><h3>“Predictability of Distribution” Across All Environments (i.e., Frequency-Only Entropy)<a class="headerlink" href="#predictability-of-distribution-across-all-environments-i-e-frequency-only-entropy" title="Permalink to this headline">¶</a></h3>
<p>Given that the calculation of predictability of distribution is based on
probabilities of occurrence across different environments, it is also
possible to calculate the overall entropy of two segments using their
raw probabilities and ignoring specific environments. Note that this
doesn’t really reveal anything about predictability of distribution per
se; it simply gives the uncertainty of occurrence of two segments that
is related to their relative frequencies. This is calculated by simply
taking the number of occurrences of each of sound 1 (N1) and sound 2
(N2) in the corpus as a whole, and then applying the following formula:</p>
<p>Formula for frequency-only entropy:</p>
<p><span class="math">\(H = (-1) * [(\frac{N1}{N1+N2}) log_{2} (\frac{N1}{N1+N2})
+(\frac{N2}{N1+N2}) log_{2} (\frac{N2}{N1+N2})]\)</span></p>
<p>The entropy will be 0 if one or both of the sounds never occur(s) in the
corpus. The entropy will be 1 if the two sounds occur with exactly the
same frequency. It will be a number between 0 and 1 if both sounds occur,
but not with the same frequency.</p>
<p>Note that an entropy of 1 in this case, which was analogous to
perfect contrast in the environment-specific implementation of this
function, does <em>not</em> align with contrast here. For example, [h] and [ŋ]
in English, which are in complementary distribution, could theoretically
have an entropy of 1 if environments are ignored and they happened to
occur with exactly the same frequency in some corpus. Similarly, two
sounds that do in fact occur in the same environments might have a low
entropy, close to 0, if one of the sounds is vastly more frequent than
the other. That is, this calculation is based ONLY on the frequency of
occurrence, and not actually on the distribution of the sounds in the
corpus. This function is thus useful only for getting a sense of the
frequency balance / imbalance between two sounds. Note that one can
also get total frequency counts for any segment in the corpus through
the “Summary” information feature (<a class="reference internal" href="loading_corpora.html#corpus-summary"><em>Summary information about a corpus</em></a>).</p>
</div>
</div>
<div class="section" id="implementing-the-predictability-of-distribution-function-in-the-gui">
<span id="pred-dist-gui"></span><h2>Implementing the predictability of distribution function in the GUI<a class="headerlink" href="#implementing-the-predictability-of-distribution-function-in-the-gui" title="Permalink to this headline">¶</a></h2>
<p>Assuming a corpus has been opened or created, predictability of
distribution is calculated using the following steps.</p>
<ol class="arabic simple">
<li><strong>Getting started</strong>: Choose “Analysis” / “Calculate predictability of
distribution...” from the top menu bar.</li>
<li><strong>Sound selection</strong>: On the left-hand side of the “Predictability of
distribution” dialogue box, select the two sounds of interest by
clicking “Add pair of sounds. The order of the sounds is
irrelevant; picking [i] first and [u] second will yield the
same results as [u] first and [i] second. Currently, PCT only
allows entire segments to be selected; the next release will allow
a “sound” to be defined as a collection of feature values. The
segment choices that are available will automatically correspond
to all of the unique transcribed characters in your corpus. You can
select more than one pair of sounds to examine in the same environments;
each pair of sounds will be treated individually.</li>
<li><strong>Environments</strong>: Click on “Add environment” to add an environment in
which to calculate predictability of distribution. The left side of
the “Create environment” dialogue box allows left-hand environments
to be specified (e.g., [+back]___), while the right side allows
right-hand environments to be specified (e.g., __#). Both can be used
simultaneously to specify environments on both sides (e.g., [+back]__#).<ol class="loweralpha">
<li><strong>Basis for building environments (segments vs. features)</strong>: Environments
can be selected either as entire segments (including #) or as bundles
of features. Select from the drop-down menu which you prefer. Each
side of an environment can be specified using either type.</li>
<li><strong>Segment selection</strong>: To specify an environment using segments, simply
click on the segment desired.</li>
<li><strong>Feature selection</strong>: To specify an environment using features, select
the first feature from the list (e.g., [voice]), and then specify
whether you want it to be [+voice] or [-voice] by selecting “Add
[+feature]” or “Add [-feature]” as relevant. To add another feature
to this same environment, select another feature and again add
either the + or – value.</li>
<li><strong>No environments</strong>: Note that if NO environments are added, PCT will
calculate the overall predictability of distribution of the two
sounds based only on their frequency of occurrence. This will simply
count the frequency of each sound in the pair and calculate the
entropy based on those frequencies (either type or token). See
below for an example of calculating environment-free entropy for
four different pairs in the sample corpus:</li>
</ol>
</li>
</ol>
<a class="reference internal image-reference" href="_images/prodfreq.png"><img alt="_images/prodfreq.png" class="align-center" src="_images/prodfreq.png" style="width: 90%;" /></a>
<ol class="arabic simple" start="4">
<li><strong>Environment list</strong>: Once all features / segments for a given environment
have been selected, for both the left- and right-hand sides, click on
“Add”; it will appear back in the “Predictability of Distribution”
dialogue box in the environment list. To automatically return to the
environment selection window to select another environment, click on
“Add and select another” instead. Individual environments from the
list can be selected and removed if it is determined that an environment
needs to be changed. It is this list that PCT will verify as being
both exhaustive and unique; i.e., the default is that the environments
on this list will exhaustively cover all instances in your corpus of
the selected sounds, but will do so in such a way that each instance
is counted exactly once.</li>
<li><strong>Analysis tier</strong>: Under “Options,” first pick the tier on which you want
predictability of distribution to be calculated. The default is for
the entire transcription to be used, such that environments are defined
on any surrounding segments. If a separate tier has been created as part
of the corpus (see <a class="reference internal" href="transcriptions_and_feature_systems.html#create-tiers"><em>Creating new tiers in the corpus</em></a>), however, predictability of distribution can
be calculated on this tier. For example, one could extract a separate
tier that contains only vowels, and then calculate predictability of
distribution based on this tier. This makes it much easier to define
non-adjacent contexts. For instance, if one wanted to investigate the
extent to which [i] and [u] are predictably distributed before front
vs. back vowels, it will be much easier to to specify that the relevant
environments are __[+back] and __[-back] on the vowel tier than to try
to account for possible intervening segments on the entire transcription
tier.</li>
<li><strong>Type vs. Token Frequency</strong>: Next, pick whether you want the calculation
to be done on types or tokens, assuming that token frequencies are
available in your corpus. If they are not, this option will not be
available. (Note: if you think your corpus does include token frequencies,
but this option seems to be unavailable, see <a class="reference internal" href="loading_corpora.html#corpus-format"><em>Required format of corpus</em></a> on the required
format for a corpus.)</li>
<li><strong>Exhaustivity &amp; Uniqueness</strong>: The default is for PCT to check for both
exhaustivity and uniqueness of environments, as described above in
<a class="reference internal" href="#pred-dist-envs"><em>Predictability of Distribution across All Environments (Systemic Entropy)</em></a>. Un-checking this box will turn off this mechanism. For
example, if you wanted to compare a series of different possible
environments, to see how the entropy calculations differ under
different generalizations, uniqueness might not be a concern. Keep
in mind that if uniqueness and exhaustivity are not met, however,
the calculation of systemic entropy will be inaccurate.<ol class="loweralpha">
<li>If you ask PCT to check for exhaustivity, and it is not met, an error
message will appear that warns you that the environments you have
selected do not exhaustively cover all instances of the symbols in
the corpus, as in the following; the “Show details...” option has
been clicked to reveal the specific words that occur in the corpus
that are not currently covered by your list of environments.
Furthermore, a .txt file is automatically created that lists all
of the words, so that the environments can be easily adjusted. This
file is stored in the ERRORS folder within the working directory
that contains the PCT software, and can be accessed directly by
clicking “Open errors directory.” If exhaustivity is not important,
and only the entropy in individual environments matters, then it is
safe to not enforce exhaustivity; it should be noted that the
weighted average entropy across environments will NOT be accurate
in this scenario, because not all words have been included.</li>
</ol>
</li>
</ol>
<a class="reference internal image-reference" href="_images/proderror2.png"><img alt="_images/proderror2.png" class="align-center" src="_images/proderror2.png" style="width: 90%;" /></a>
<p>Here’s an example of correctly exhaustive and unique selections for
calculating the predictability of distribution based on token frequency
for [s] and [ʃ] in the sample corpus:</p>
<a class="reference internal image-reference" href="_images/proddialog.png"><img alt="_images/proddialog.png" class="align-center" src="_images/proddialog.png" style="width: 90%;" /></a>
<ol class="arabic simple" start="8">
<li><strong>Entropy calculation / results</strong>: Once all environments have been specified,
click “Calculate predictability of distribution.” If you want to start
a new results table, click that button; if you’ve already done at least
one calculation and want to add new calculations to the same table,
select the button with “add to current results table.” Results will
appear in a pop-up window on screen.  The last row for each pair gives
the weighted average entropy across all selected environments, with
the environments being weighted by their own frequency of occurrence.
See the following example:</li>
</ol>
<a class="reference internal image-reference" href="_images/prodresults.png"><img alt="_images/prodresults.png" class="align-center" src="_images/prodresults.png" style="width: 90%;" /></a>
<ol class="arabic simple" start="9">
<li><strong>Output file / Saving results</strong>: If you want to save the table of results,
click on “Save to file” at the bottom of the table. This opens up a
system dialogue box where the directory and name can be selected.</li>
</ol>
<p>To return to the function dialogue box with your most recently used
selections, click on “Reopen function dialog.” Otherwise, the results
table can be closed and you will be returned to your corpus view.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Predictability of Distribution</a><ul>
<li><a class="reference internal" href="#about-the-function">About the function</a></li>
<li><a class="reference internal" href="#method-of-calculation">Method of calculation</a><ul>
<li><a class="reference internal" href="#predictability-of-distribution-in-a-single-environment">Predictability of Distribution in a Single Environment</a></li>
<li><a class="reference internal" href="#predictability-of-distribution-across-all-environments-systemic-entropy">Predictability of Distribution across All Environments (Systemic Entropy)</a></li>
<li><a class="reference internal" href="#predictability-of-distribution-across-all-environments-i-e-frequency-only-entropy">“Predictability of Distribution” Across All Environments (i.e., Frequency-Only Entropy)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementing-the-predictability-of-distribution-function-in-the-gui">Implementing the predictability of distribution function in the GUI</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="functional_load.html"
                        title="previous chapter">Functional Load</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="kullback-leibler.html"
                        title="next chapter">Kullback-Leibler Divergence</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/predictability_of_distribution.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="kullback-leibler.html" title="Kullback-Leibler Divergence"
             >next</a> |</li>
        <li class="right" >
          <a href="functional_load.html" title="Functional Load"
             >previous</a> |</li>
        <li><a href="index.html">Phonological CorpusTools 1.0.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2015, PCT.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>